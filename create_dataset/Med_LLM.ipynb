{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f3c4df-4690-4a24-981d-1ac612943f1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MedPalm\n",
      "  Using cached medpalm-0.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting SentencePiece (from MedPalm)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting einops (from MedPalm)\n",
      "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch (from MedPalm)\n",
      "  Using cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers (from MedPalm)\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting zetascale (from MedPalm)\n",
      "  Using cached zetascale-2.2.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->MedPalm) (3.6.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch->MedPalm)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->MedPalm) (1.10.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->MedPalm) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->MedPalm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->MedPalm) (2022.7.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->MedPalm)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->MedPalm)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->MedPalm)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->MedPalm)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->MedPalm)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->MedPalm)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->MedPalm)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->MedPalm)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->MedPalm)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->MedPalm)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->MedPalm)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch->MedPalm)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->MedPalm)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers->MedPalm)\n",
      "  Using cached huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->MedPalm) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->MedPalm) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers->MedPalm) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->MedPalm) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->MedPalm) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers->MedPalm)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->MedPalm)\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->MedPalm) (4.64.1)\n",
      "Collecting accelerate==0.27.2 (from zetascale->MedPalm)\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting argparse<2.0.0,>=1.4.0 (from zetascale->MedPalm)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting beartype==0.17.2 (from zetascale->MedPalm)\n",
      "  Using cached beartype-0.17.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting bitsandbytes==0.42.0 (from zetascale->MedPalm)\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting colt5-attention (from zetascale->MedPalm)\n",
      "  Using cached CoLT5_attention-0.10.20-py3-none-any.whl.metadata (738 bytes)\n",
      "Collecting datasets (from zetascale->MedPalm)\n",
      "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting einops-exts==0.0.4 (from zetascale->MedPalm)\n",
      "  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting lion-pytorch==0.1.2 (from zetascale->MedPalm)\n",
      "  Using cached lion_pytorch-0.1.2-py3-none-any.whl.metadata (620 bytes)\n",
      "Collecting loguru (from zetascale->MedPalm)\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.10/site-packages (from zetascale->MedPalm) (2.8.3)\n",
      "Collecting pytest==8.0.2 (from zetascale->MedPalm)\n",
      "  Using cached pytest-8.0.2-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: rich==13.7.0 in /opt/conda/lib/python3.10/site-packages (from zetascale->MedPalm) (13.7.0)\n",
      "Collecting scipy==1.9.3 (from zetascale->MedPalm)\n",
      "  Using cached scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting skypilot==0.4.1 (from zetascale->MedPalm)\n",
      "  Using cached skypilot-0.4.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tiktoken==0.6.0 (from zetascale->MedPalm)\n",
      "  Using cached tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting timm==0.9.16 (from zetascale->MedPalm)\n",
      "  Using cached timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tokenmonster==1.1.12 (from zetascale->MedPalm)\n",
      "  Using cached tokenmonster-1.1.12-py3-none-any.whl\n",
      "Collecting torch (from MedPalm)\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchdiffeq==0.2.3 (from zetascale->MedPalm)\n",
      "  Using cached torchdiffeq-0.2.3-py3-none-any.whl.metadata (488 bytes)\n",
      "Collecting torchfix (from zetascale->MedPalm)\n",
      "  Using cached TorchFix-0.4.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting torchvision==0.17.0 (from zetascale->MedPalm)\n",
      "  Using cached torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers->MedPalm)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting transformers (from MedPalm)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting typing==3.7.4.3 (from zetascale->MedPalm)\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Collecting vector-quantize-pytorch==1.14.1 (from zetascale->MedPalm)\n",
      "  Using cached vector_quantize_pytorch-1.14.1-py3-none-any.whl.metadata (716 bytes)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2->zetascale->MedPalm) (5.9.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest==8.0.2->zetascale->MedPalm) (1.1.1)\n",
      "Collecting pluggy<2.0,>=1.3.0 (from pytest==8.0.2->zetascale->MedPalm)\n",
      "  Using cached pluggy-1.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest==8.0.2->zetascale->MedPalm) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pytest==8.0.2->zetascale->MedPalm) (2.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->zetascale->MedPalm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich==13.7.0->zetascale->MedPalm) (2.17.2)\n",
      "Collecting numpy>=1.17 (from transformers->MedPalm)\n",
      "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (0.42.0)\n",
      "Collecting cachetools (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.4.5 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (0.4.4)\n",
      "Requirement already satisfied: cryptography in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (41.0.7)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (4.20.0)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (2.1.3)\n",
      "Collecting pendulum (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting PrettyTable>=2.0.0 (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached prettytable-3.10.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting python-dotenv (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (0.8.10)\n",
      "Collecting pulp (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached PuLP-2.8.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting urllib3<2 (from skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: awscli>=1.27.10 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (1.31.9)\n",
      "Requirement already satisfied: botocore>=1.29.10 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (1.33.9)\n",
      "Requirement already satisfied: boto3>=1.26.1 in /opt/conda/lib/python3.10/site-packages (from skypilot==0.4.1->zetascale->MedPalm) (1.33.9)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.17.0->zetascale->MedPalm) (10.1.0)\n",
      "Collecting einx>=0.1.3 (from einx[torch]>=0.1.3->vector-quantize-pytorch==1.14.1->zetascale->MedPalm)\n",
      "  Using cached einx-0.1.3-py3-none-any.whl\n",
      "Collecting fsspec (from torch->MedPalm)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->MedPalm) (2.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers->MedPalm) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->MedPalm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->MedPalm) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->MedPalm) (2023.11.17)\n",
      "Collecting local-attention>=1.8.6 (from colt5-attention->zetascale->MedPalm)\n",
      "  Using cached local_attention-1.9.0-py3-none-any.whl.metadata (682 bytes)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->zetascale->MedPalm) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets->zetascale->MedPalm)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->zetascale->MedPalm) (0.3.7)\n",
      "Collecting xxhash (from datasets->zetascale->MedPalm)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->zetascale->MedPalm) (0.70.15)\n",
      "Collecting fsspec (from torch->MedPalm)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets->zetascale->MedPalm)\n",
      "  Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->MedPalm) (1.3.0)\n",
      "Requirement already satisfied: flake8>=3.8.2 in /opt/conda/lib/python3.10/site-packages (from torchfix->zetascale->MedPalm) (4.0.1)\n",
      "Collecting libcst<1.2.0,>=1.1.0 (from torchfix->zetascale->MedPalm)\n",
      "  Using cached libcst-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /opt/conda/lib/python3.10/site-packages (from awscli>=1.27.10->skypilot==0.4.1->zetascale->MedPalm) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from awscli>=1.27.10->skypilot==0.4.1->zetascale->MedPalm) (0.8.2)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from awscli>=1.27.10->skypilot==0.4.1->zetascale->MedPalm) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.10->skypilot==0.4.1->zetascale->MedPalm) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.10->skypilot==0.4.1->zetascale->MedPalm) (2.8.2)\n",
      "Collecting frozendict (from einx>=0.1.3->einx[torch]>=0.1.3->vector-quantize-pytorch==1.14.1->zetascale->MedPalm)\n",
      "  Using cached frozendict-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8>=3.8.2->torchfix->zetascale->MedPalm)\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from flake8>=3.8.2->torchfix->zetascale->MedPalm) (2.8.0)\n",
      "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from flake8>=3.8.2->torchfix->zetascale->MedPalm) (2.4.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->zetascale->MedPalm)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->zetascale->MedPalm) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->zetascale->MedPalm)\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->zetascale->MedPalm)\n",
      "  Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->zetascale->MedPalm)\n",
      "  Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->zetascale->MedPalm)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0 (from libcst<1.2.0,>=1.1.0->torchfix->zetascale->MedPalm)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich==13.7.0->zetascale->MedPalm) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->skypilot==0.4.1->zetascale->MedPalm) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.3.0->skypilot==0.4.1->zetascale->MedPalm) (2023.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from PrettyTable>=2.0.0->skypilot==0.4.1->zetascale->MedPalm) (0.2.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography->skypilot==0.4.1->zetascale->MedPalm) (1.15.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->skypilot==0.4.1->zetascale->MedPalm) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->skypilot==0.4.1->zetascale->MedPalm) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->skypilot==0.4.1->zetascale->MedPalm) (0.13.2)\n",
      "Collecting time-machine>=2.6.0 (from pendulum->skypilot==0.4.1->zetascale->MedPalm)\n",
      "  Using cached time_machine-2.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography->skypilot==0.4.1->zetascale->MedPalm) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.10->skypilot==0.4.1->zetascale->MedPalm) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli>=1.27.10->skypilot==0.4.1->zetascale->MedPalm) (0.4.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->libcst<1.2.0,>=1.1.0->torchfix->zetascale->MedPalm) (0.4.3)\n",
      "Using cached medpalm-0.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Using cached zetascale-2.2.6-py3-none-any.whl (491 kB)\n",
      "Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Using cached beartype-0.17.2-py3-none-any.whl (872 kB)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Using cached einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
      "Using cached lion_pytorch-0.1.2-py3-none-any.whl (4.4 kB)\n",
      "Using cached pytest-8.0.2-py3-none-any.whl (333 kB)\n",
      "Using cached scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
      "Using cached skypilot-0.4.1-py3-none-any.whl (746 kB)\n",
      "Using cached tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Using cached timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
      "Using cached torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
      "Using cached torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached vector_quantize_pytorch-1.14.1-py3-none-any.whl (28 kB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Using cached huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\n",
      "Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached CoLT5_attention-0.10.20-py3-none-any.whl (18 kB)\n",
      "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Using cached TorchFix-0.4.0-py3-none-any.whl (24 kB)\n",
      "Using cached aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached libcst-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n",
      "Using cached pluggy-1.4.0-py3-none-any.whl (20 kB)\n",
      "Using cached prettytable-3.10.0-py3-none-any.whl (28 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached pendulum-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "Using cached PuLP-2.8.0-py3-none-any.whl (17.7 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Using cached multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Using cached time_machine-2.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Using cached frozendict-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n",
      "Installing collected packages: tokenmonster, SentencePiece, mccabe, argparse, xxhash, urllib3, typing-extensions, typing, triton, tqdm, safetensors, python-dotenv, pyarrow-hotfix, pulp, PrettyTable, pluggy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, loguru, fsspec, frozenlist, frozendict, einops, cachetools, beartype, async-timeout, yarl, typing-inspect, time-machine, scipy, pytest, nvidia-cusparse-cu12, nvidia-cudnn-cu12, einx, einops-exts, aiosignal, tiktoken, pendulum, nvidia-cusolver-cu12, libcst, huggingface-hub, bitsandbytes, aiohttp, torchfix, torch, tokenizers, transformers, torchvision, torchdiffeq, skypilot, local-attention, lion-pytorch, datasets, accelerate, vector-quantize-pytorch, timm, colt5-attention, zetascale, MedPalm\n",
      "  Attempting uninstall: mccabe\n",
      "    Found existing installation: mccabe 0.7.0\n",
      "    Uninstalling mccabe-0.7.0:\n",
      "      Successfully uninstalled mccabe-0.7.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.0.0\n",
      "    Uninstalling pluggy-1.0.0:\n",
      "      Successfully uninstalled pluggy-1.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.2\n",
      "    Uninstalling numpy-1.26.2:\n",
      "      Successfully uninstalled numpy-1.26.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.4\n",
      "    Uninstalling scipy-1.11.4:\n",
      "      Successfully uninstalled scipy-1.11.4\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.4.3\n",
      "    Uninstalling pytest-7.4.3:\n",
      "      Successfully uninstalled pytest-7.4.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.18.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MedPalm-0.2.0 PrettyTable-3.10.0 SentencePiece-0.2.0 accelerate-0.27.2 aiohttp-3.9.3 aiosignal-1.3.1 argparse-1.4.0 async-timeout-4.0.3 beartype-0.17.2 bitsandbytes-0.42.0 cachetools-5.3.3 colt5-attention-0.10.20 datasets-2.18.0 einops-0.7.0 einops-exts-0.0.4 einx-0.1.3 frozendict-2.4.0 frozenlist-1.4.1 fsspec-2024.2.0 huggingface-hub-0.22.1 libcst-1.1.0 lion-pytorch-0.1.2 local-attention-1.9.0 loguru-0.7.2 mccabe-0.6.1 multidict-6.0.5 numpy-1.25.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pendulum-3.0.0 pluggy-1.4.0 pulp-2.8.0 pyarrow-hotfix-0.6 pytest-8.0.2 python-dotenv-1.0.1 safetensors-0.4.2 scipy-1.9.3 skypilot-0.4.1 tiktoken-0.6.0 time-machine-2.14.1 timm-0.9.16 tokenizers-0.15.2 tokenmonster-1.1.12 torch-2.2.0 torchdiffeq-0.2.3 torchfix-0.4.0 torchvision-0.17.0 tqdm-4.66.2 transformers-4.38.2 triton-2.2.0 typing-3.7.4.3 typing-extensions-4.10.0 typing-inspect-0.9.0 urllib3-1.26.18 vector-quantize-pytorch-1.14.1 xxhash-3.4.1 yarl-1.9.4 zetascale-2.2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install MedPalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160bcb03-304a-45d8-b09e-55a6ef3e6738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.34.0\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from transformers==4.34.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (4.66.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.34.0) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2023.11.17)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.0\n",
      "    Uninstalling transformers-4.30.0:\n",
      "      Successfully uninstalled transformers-4.30.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "zetascale 2.2.6 requires transformers==4.38.2, but you have transformers 4.34.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.14.1 transformers-4.34.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install transformers==4.34.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15bfcedc-a6d4-4c50-8eaa-b99c3e69b115",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.9.3)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73188730-280d-4335-8829-97f75eb8d179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.9.3)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaaaeac-dcc2-4d63-9c09-ab4eaaab5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "q11 = \"Give me the current blood pressure of the patient 00001\"\n",
    "q12 = \"Give me the respository rate of the patient 00001 in yesterday afternoon\"\n",
    "q13 = \"Provide me the heart rate of the patient 00001 in the last 3 days\"\n",
    "\n",
    "q21 = \"Give me the current vital signs of the patient 00001\"\n",
    "q22 = \"Give me the respository rate and heart rate of the patient 00001 from 8am to 2pm.\"\n",
    "q23 = \"Provide me the heart rate and temperature of the patient 00001 in the last 3 days\"\n",
    "\n",
    "q31 = \"Show me the current image of the patient 00001\"\n",
    "q32 = \"Give me the image of the patient 00001 in yesterday morning\"\n",
    "q33 = \"Give me the image of the patient 00001 in the morning of the last 3 days\"\n",
    "\n",
    "q41 = \"What is the patient 00001 doing and what is the patient's mood now?\"\n",
    "q42 = \"What is the patient 00001 doing and what is the patient's mood at 2pm?\"\n",
    "q43 = \"What is the patient 00001 doing and what is the patient's mood in the last three days?\"\n",
    "\n",
    "q51 = \"Give me a plot of patient 00001 heart rate from 8am to 2pm.\"\n",
    "q52 = \"Give me a plot of patient 00001 oxygen saturation and body temperature from 8am to 2pm.\"\n",
    "q53 = \"Give me a plot of patient 00001 heart rate last week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cfdb8e-997a-416d-8a1a-b47f749dc93b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7cc89-7481-48e2-b1b6-ab9baffbfe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"epfl-llm/meditron-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"Hello BiMediX! I've been experiencing increased tiredness in the past week.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03ffcc-5894-4339-aa42-c0056165f2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c5d73-127e-44f6-ace5-c7d66fabebee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874600d-9239-4224-907b-7a3d0f204da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858376b-6f6e-47fc-a41f-331524217b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27e1e831-1bb4-41c9-81d1-b3d35bff3409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = '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')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "119eff8e-2e14-44cf-ac5b-8245db4934b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3a436094e74261a0916b061d134cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer,\n",
    ")\n",
    "import accelerate \n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'BioMistral/BioMistral-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id    \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_compute_dtype=torch.float16,\n",
    "    #bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"balanced\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#tokenizer.pad_token_id = tokenizer.eos_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f171ee-98a6-4794-b9e1-9b0971ff365d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ed7e9-04c1-4c4a-a06d-17abc91056cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# Move inputs to GPU\n",
    "#input_ids = torch.as_tensor(inputs.input_ids).to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        clip_features= clip_features,\n",
    "        temperature=0.5,\n",
    "        top_k=50,\n",
    "        top_p=1.0,\n",
    "        max_new_tokens=512,\n",
    "        repetition_penalty=1.1,\n",
    "        early_stopping=True,\n",
    "        use_cache=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "        \n",
    "# Decode output tokens\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    \n",
    "# Clean output string\n",
    "output = output.strip()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f2d7dfd-48b2-4355-90fd-dfb65206be9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response_for_patient(ques, model, tokenizer):\n",
    "    #call = patient_intent_detection(ques)\n",
    "    #if call == 1:\n",
    "    #    output = \"I am reaching out to your caregiver!\"\n",
    "    #    return output, call\n",
    "    \n",
    "    # define streamer and pipeline\n",
    "    generation_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"balanced\",    # finds GPU\n",
    "    )\n",
    "    \n",
    "    formatted_prompt = '''\n",
    "    The current time is 2024-03-26 01:30:15.\n",
    "    You are a helpful assistant. Your task is to detect the user's intent and provide a response in the form of a JSON object complete with the following keys:\n",
    "    'patient_id': A string representing the ID of the patient the user is inquiring about. If patient ID is not provided, leave it empty.\n",
    "    'list_date': A list of dates for which data needs to be retrieved to answer the user's question in format of yyyy-mm-dd. Leave the list empty if the user asks for data right now.\n",
    "    'list_time':  A list of times for which data needs to be retrieved to answer the user's question in format of hh:mm:ss. The system saves data in 30-minute period like 00:00:00 and 00:30:00. If the user asks for sessions during the day, please use the following information to fill in the list: Morning is from 5am to 12pm, Afternoon is from 12pm to 5pm, Evening is from 5pm to 9pm, Night is from 9pm to 4am. Leave the list empty if the user asks for data right now. \n",
    "    'vital_sign': A list of vital signs that the user is asking for. Here are the available vital sign: heart_rate, systolic_pressure, diastolic_pressure, respiratory_rate, body_temperature, oxygen_saturation\n",
    "    'is_plot': A Boolean value indicating whether the system needs to generate a plot to answer the question more clearly (when the number of data points is too large) or if the user has requested a plot.\n",
    "    'recognition': A Boolean value indicating whether the user is asking for activity or emotion recognition of the patient. Set to true if the user is asking for this information, otherwise false.\n",
    "    'is_image': A Boolean value indicating whether the user is asking to show an image of the patient.\n",
    "    \n",
    "    ### Input: Give me the current blood pressure of the patient 00001 in yesterday morning from 1pm to 2pm\n",
    "    ### Response:\n",
    "    {\n",
    "      'patient_id': '00001',\n",
    "      'list_date': ['2024-03-25'],\n",
    "      'list_time': ['13:00:00', '13:30:00', '14:00:00'],\n",
    "      'vital_sign': [\"systolic_pressure\", \"diastolic_pressure\"],\n",
    "      'is_plot': false,\n",
    "      'recognition': false,\n",
    "      'is_image': false\n",
    "    }\n",
    "    \n",
    "    ### Input: Give me the image of the patient 00001 in yesterday morning\n",
    "    ### Response:\n",
    "    '''\n",
    "    \n",
    "\n",
    "    sequences = generation_pipe(\n",
    "        formatted_prompt,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        #repetition_penalty=1.2,\n",
    "        #top_k=50,\n",
    "        temperature=0.1,\n",
    "        #top_p=1.0,\n",
    "        max_new_tokens=500,\n",
    "        #early_stopping=True,\n",
    "        use_cache=True,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    text_without_prompt = sequences[0][\"generated_text\"][len(formatted_prompt):]\n",
    "    #print(text_without_prompt)\n",
    "    #output = filter_sentences(text_without_prompt)\n",
    "    output = text_without_prompt.strip()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24dd579a-f9b0-4d88-87a8-4fa3efb1d2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "      'patient_id': '00001',\n",
      "      'list_date': ['2024-03-25'],\n",
      "      'list_time': ['13:00:00', '13:30:00', '14:00:00'],\n",
      "      'vital_sign': [\"systolic_pressure\", \"diastolic_pressure\"],\n"
     ]
    }
   ],
   "source": [
    "ques = \"\"\n",
    "output = get_response_for_patient(ques, model, tokenizer)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c16a19b2-5b93-442c-9a65-55e934897167",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 2 column 8 (char 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m intent_dict \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m intent_dict\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 2 column 8 (char 9)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "intent_dict = json.loads(output)\n",
    "intent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee014c4-a95f-4216-a746-96f725f33cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3a117-265c-47f1-a224-a53178e892e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a4b79c-0bc2-407c-b61d-07853a12d0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc       \n",
    "model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eca034a7-acbe-4bb3-9855-faf5b22e315d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 26 21:42:44 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P0              56W / 300W |  14186MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#del model\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f850370-8810-4114-96a2-ff452fec3624",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (296235269.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \\n\\n### Input: Give me the current blood pressure of the patient 00001\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "\\n\\n### Input: Give me the current blood pressure of the patient 00001\n",
    "    \\n\\n### Response:\n",
    "    {\n",
    "      'patient_id': '00001',\n",
    "      'list_date': [],\n",
    "      'list_time': [],\n",
    "      'vital_sign': [\"systolic_pressure\", \"diastolic_pressure\"],\n",
    "      'is_plot': false,\n",
    "      'recognition': false,\n",
    "      'is_image': false\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29753e2-8ff4-4b05-a209-3005af548163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here is an example: \n",
    "    Input: Give me the current blood pressure of the patient 00001 in yesterday morning from 1pm to 2pm\n",
    "    Response:\n",
    "    {\n",
    "      'patient_id': '00001',\n",
    "      'list_date': ['2024-03-25'],\n",
    "      'list_time': ['13:00:00', '13:30:00', '14:00:00'],\n",
    "      'vital_sign': [\"systolic_pressure\", \"diastolic_pressure\"],\n",
    "      'is_plot': false,\n",
    "      'recognition': false,\n",
    "      'is_image': false\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae70be-b2a2-474c-83b4-5f9a7264dc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4c1de-f6a2-41b5-a0e2-41e6671a1f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34297249-26f4-4343-ac52-b72a90ba44ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5e9c7-311e-4952-843e-0429a3d41ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

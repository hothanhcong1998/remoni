{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU is available.\n",
      "Using device: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from huggingface_hub import login\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"NVIDIA GPU is available.\")\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"NVIDIA GPU is not available.\")\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:47<00:00,  9.42s/it]\n"
     ]
    }
   ],
   "source": [
    "access_token = ''\n",
    "# Log in with your Hugging Face token\n",
    "login(token=access_token)\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token = access_token\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "#url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "#image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: requests in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_INTENT_DETECTION = \"\"\"\n",
    "The current time is 2024-01-15 03:47:43.\n",
    "\n",
    "You are a helpful assistant. Your task is to detect the user's intent and provide a response in the form of a JSON object complete with the following keys:\n",
    "1. 'patient_id': A string representing the ID of the patient the user is inquiring about. If patient ID is not provided, leave it empty.\n",
    "2. 'list_date': A list of dates for which data needs to be retrieved to answer the user's question in format of yyyy-mm-dd. Leave the list empty if the user asks for data right now.\n",
    "3. 'list_time':  A list of times for which data needs to be retrieved to answer the user's question in format of hh:mm:ss. The system saves data in 30-minute period like 00:00:00 and 00:30:00. Leave the list empty if the user asks for data right now.\n",
    "4. 'vital_sign': A list of vital signs that the user is asking for. Here are the available vital sign: heart_rate, systolic_pressure, diastolic_pressure, respiratory_rate, body_temperature, oxygen_saturation\n",
    "5. 'is_plot': A Boolean value indicating whether the system needs to generate a plot to answer the question more clearly (when the number of data points is too large) or if the user has requested a plot.\n",
    "6. 'recognition': A Boolean value indicating whether the user is asking for activity or emotion recognition of the patient. Set to true if the user is asking for this information, otherwise false.\n",
    "7. 'is_image': A Boolean value indicating whether the user is asking to show an image of the patient.\n",
    "\n",
    "If the user provides the time in both AM and PM formats, you can still interpret it in the 'hh:mm:ss' format.\n",
    "If the user asks for sessions during the day, please use the following information to fill in list_time: Morning is from 05:00:00 (5am) to 12:00:00 (12pm), Afternoon is from 12:00:00 (12pm) to 17:00:00 (5pm), Evening is from 17:00:00 (5pm) to 21:00:00 (9pm), Night is from 21:00:00 (9pm) to 04:00:00 (4am) the following day.\n",
    "It's important to remember that health status consists of vital signs.\n",
    "When the user asks for data from the last a number of hours, today should also be included in the list_date.\n",
    "When the user asks for data last morning, last afternoon, last evening, last night, fill the previous day and today in list_date.\n",
    "When users ask questions related to time periods such as last month, last hours, last days, or last week, try your best to fill in list_date and list_time with full dates and times within the specified range.\n",
    "\n",
    "Provide me the respiratory rate and systolic pressure of the patient 05419 in the last 19 days.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "The current time is 2024-01-15 03:47:43.\n",
    "\n",
    "You are a helpful assistant. Your task is to detect the user's intent and provide a response in the form of a JSON object complete with the following keys:\n",
    "1. 'patient_id': A string representing the ID of the patient the user is inquiring about. If patient ID is not provided, leave it empty.\n",
    "2. 'list_date': A list of dates for which data needs to be retrieved to answer the user's question in format of yyyy-mm-dd. Leave the list empty if the user asks for data right now.\n",
    "3. 'list_time':  A list of times for which data needs to be retrieved to answer the user's question in format of hh:mm:ss. The system saves data in 30-minute period like 00:00:00 and 00:30:00. Leave the list empty if the user asks for data right now.\n",
    "4. 'vital_sign': A list of vital signs that the user is asking for. Here are the available vital sign: heart_rate, systolic_pressure, diastolic_pressure, respiratory_rate, body_temperature, oxygen_saturation\n",
    "5. 'is_plot': A Boolean value indicating whether the system needs to generate a plot to answer the question more clearly (when the number of data points is too large) or if the user has requested a plot.\n",
    "6. 'recognition': A Boolean value indicating whether the user is asking for activity or emotion recognition of the patient. Set to true if the user is asking for this information, otherwise false.\n",
    "7. 'is_image': A Boolean value indicating whether the user is asking to show an image of the patient.\n",
    "\n",
    "If the user provides the time in both AM and PM formats, you can still interpret it in the 'hh:mm:ss' format.\n",
    "If the user asks for sessions during the day, please use the following information to fill in list_time: Morning is from 05:00:00 (5am) to 12:00:00 (12pm), Afternoon is from 12:00:00 (12pm) to 17:00:00 (5pm), Evening is from 17:00:00 (5pm) to 21:00:00 (9pm), Night is from 21:00:00 (9pm) to 04:00:00 (4am) the following day.\n",
    "It's important to remember that health status consists of vital signs.\n",
    "When the user asks for data from the last a number of hours, today should also be included in the list_date.\n",
    "When the user asks for data last morning, last afternoon, last evening, last night, fill the previous day and today in list_date.\n",
    "When users ask questions related to time periods such as last month, last hours, last days, or last week, try your best to fill in list_date and list_time with full dates and times within the specified range.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Provide me the respiratory rate and systolic pressure of the patient 05419 in the last 19 days.<|eot_id|><|start_header_id|>assistant<|end_header_id|> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh.ho/.conda/envs/taco/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(None, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=1000, early_stopping=True)\n",
    "full_response = processor.decode(output[0])\n",
    "text_without_prompt = full_response[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|python_tag|>{\n",
      "  \"patient_id\": \"05419\",\n",
      "  \"list_date\": [\n",
      "    \"2023-12-27\",\n",
      "    \"2023-12-28\",\n",
      "    \"2023-12-29\",\n",
      "    \"2023-12-30\",\n",
      "    \"2023-12-31\",\n",
      "    \"2024-01-01\",\n",
      "    \"2024-01-02\",\n",
      "    \"2024-01-03\",\n",
      "    \"2024-01-04\",\n",
      "    \"2024-01-05\",\n",
      "    \"2024-01-06\",\n",
      "    \"2024-01-07\",\n",
      "    \"2024-01-08\",\n",
      "    \"2024-01-09\",\n",
      "    \"2024-01-10\",\n",
      "    \"2024-01-11\",\n",
      "    \"2024-01-12\",\n",
      "    \"2024-01-13\",\n",
      "    \"2024-01-14\"\n",
      "  ],\n",
      "  \"list_time\": [],\n",
      "  \"vital_sign\": [\n",
      "    \"respiratory_rate\",\n",
      "    \"systolic_pressure\"\n",
      "  ],\n",
      "  \"is_plot\": false,\n",
      "  \"recognition\": false,\n",
      "  \"is_image\": false\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I am not able to provide a response to that request.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "  \"patient_id\": \"05419\",\n",
      "  \"list_date\": [\n",
      "    \"2023-12-27\",\n",
      "    \"2023-12-28\",\n",
      "    \"2023-12-29\",\n",
      "    \"2023-12-30\",\n",
      "    \"2023-12-31\",\n",
      "    \"2024-01-01\",\n",
      "    \"2024-01-02\",\n",
      "    \"2024-01-03\",\n",
      "    \"2024-01-04\",\n",
      "    \"2024-01-05\",\n",
      "    \"2024-01-06\",\n",
      "    \"2024-01-07\",\n",
      "    \"2024-01-08\",\n",
      "    \"2024-01-09\",\n",
      "    \"2024-01-10\",\n",
      "    \"2024-01-11\",\n",
      "    \"2024-01-12\",\n",
      "    \"2024-01-13\",\n",
      "    \"2024-01-14\"\n",
      "  ],\n",
      "  \"list_time\": [],\n",
      "  \"vital_sign\": [\n",
      "    \"respiratory_rate\",\n",
      "    \"systolic_pressure\"\n",
      "  ],\n",
      "  \"is_plot\": false,\n",
      "  \"recognition\": false,\n",
      "  \"is_image\": false\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "  \"patient_id\": \"05419\",\n",
      "  \"list_date\": [\n",
      "    \"2024-01-01\",\n",
      "    \"2024-01-02\",\n",
      "    \"2024-01-03\",\n",
      "    \"2024-01-04\",\n",
      "    \"2024-01-05\",\n",
      "    \"2024-01-06\",\n",
      "    \"2024-01-07\",\n",
      "    \"2024-01-08\",\n",
      "    \"2024-01-09\",\n",
      "    \"2024-01-10\",\n",
      "    \"2024-01-11\",\n",
      "    \"2024-01-12\",\n",
      "    \"2024-01-13\",\n",
      "    \"2024-01-14\",\n",
      "    \"2024-01-15\",\n",
      "    \"2024-01-16\",\n",
      "    \"2024-01-17\",\n",
      "    \"2024-01-18\",\n",
      "    \"2024-01-19\"\n",
      "  ],\n",
      "  \"list_time\": [],\n",
      "  \"vital_sign\": [\n",
      "    \"respiratory_rate\",\n",
      "    \"systolic_pressure\"\n",
      "  ],\n",
      "  \"is_plot\": false,\n",
      "  \"recognition\": false,\n",
      "  \"is_image\": false\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "  \"patient_id\": \"05419\",\n",
      "  \"list_date\": [\n",
      "    \"2024-01-01\",\n",
      "    \"2024-01-02\",\n",
      "    \"2024-01-03\",\n",
      "    \"2024-01-04\",\n",
      "    \"2024-01-05\",\n",
      "    \"2024-01-06\",\n",
      "    \"2024-01-07\",\n",
      "    \"2024-01-08\",\n",
      "    \"2024-01-09\",\n",
      "    \"2024-01-10\",\n",
      "    \"2024-01-11\",\n",
      "    \"2024-01-12\",\n",
      "    \"2024-01-13\",\n",
      "    \"2024-01-14\",\n",
      "    \"2024-01-15\",\n",
      "    \"2024-01-16\",\n",
      "    \"2024-01-17\",\n",
      "    \"2024-01-18\",\n",
      "    \"2024-01-19\"\n",
      "  ],\n",
      "  \"list_time\": [],\n",
      "  \"vital_sign\": [\n",
      "    \"respiratory_rate\",\n",
      "    \"systolic_pressure\"\n",
      "  ],\n",
      "  \"is_plot\": false,\n",
      "  \"recognition\": false,\n",
      "  \"is_image\": false\n",
      "}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_without_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  4.60it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'apply_chat_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m      5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      9\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    <|start_header_id|>system<|end_header_id|>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m    Provide me the respiratory rate and systolic pressure of the patient 05419 in the last 19 days.<|eot_id|><|start_header_id|>assistant<|end_header_id|> \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     }]\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipe(\n\u001b[1;32m     38\u001b[0m     messages,\n\u001b[1;32m     39\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m response \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/.conda/envs/taco/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:278\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(Chat(text_inputs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/taco/lib/python3.12/site-packages/transformers/pipelines/base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         )\n\u001b[1;32m   1360\u001b[0m     )\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.conda/envs/taco/lib/python3.12/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1368\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1369\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1370\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/.conda/envs/taco/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:314\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_final_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m         continue_final_message \u001b[38;5;241m=\u001b[39m prompt_text\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m    315\u001b[0m         prompt_text\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    316\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m continue_final_message,\n\u001b[1;32m    317\u001b[0m         continue_final_message\u001b[38;5;241m=\u001b[39mcontinue_final_message,\n\u001b[1;32m    318\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    319\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework,\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs,\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prefix \u001b[38;5;241m+\u001b[39m prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'apply_chat_template'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    The current time is 2024-01-15 03:47:43.\n",
    "\n",
    "    You are a helpful assistant. Your task is to detect the user's intent and provide a response in the form of a JSON object complete with the following keys:\n",
    "    1. 'patient_id': A string representing the ID of the patient the user is inquiring about. If patient ID is not provided, leave it empty.\n",
    "    2. 'list_date': A list of dates for which data needs to be retrieved to answer the user's question in format of yyyy-mm-dd. Leave the list empty if the user asks for data right now.\n",
    "    3. 'list_time':  A list of times for which data needs to be retrieved to answer the user's question in format of hh:mm:ss. The system saves data in 30-minute period like 00:00:00 and 00:30:00. Leave the list empty if the user asks for data right now.\n",
    "    4. 'vital_sign': A list of vital signs that the user is asking for. Here are the available vital sign: heart_rate, systolic_pressure, diastolic_pressure, respiratory_rate, body_temperature, oxygen_saturation\n",
    "    5. 'is_plot': A Boolean value indicating whether the system needs to generate a plot to answer the question more clearly (when the number of data points is too large) or if the user has requested a plot.\n",
    "    6. 'recognition': A Boolean value indicating whether the user is asking for activity or emotion recognition of the patient. Set to true if the user is asking for this information, otherwise false.\n",
    "    7. 'is_image': A Boolean value indicating whether the user is asking to show an image of the patient.\n",
    "\n",
    "    If the user provides the time in both AM and PM formats, you can still interpret it in the 'hh:mm:ss' format.\n",
    "    If the user asks for sessions during the day, please use the following information to fill in list_time: Morning is from 05:00:00 (5am) to 12:00:00 (12pm), Afternoon is from 12:00:00 (12pm) to 17:00:00 (5pm), Evening is from 17:00:00 (5pm) to 21:00:00 (9pm), Night is from 21:00:00 (9pm) to 04:00:00 (4am) the following day.\n",
    "    It's important to remember that health status consists of vital signs.\n",
    "    When the user asks for data from the last a number of hours, today should also be included in the list_date.\n",
    "    When the user asks for data last morning, last afternoon, last evening, last night, fill the previous day and today in list_date.\n",
    "    When users ask questions related to time periods such as last month, last hours, last days, or last week, try your best to fill in list_date and list_time with full dates and times within the specified range.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Provide me the respiratory rate and systolic pressure of the patient 05419 in the last 19 days.<|eot_id|><|start_header_id|>assistant<|end_header_id|> \"\"\"\n",
    "    }]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(response)\n",
    "# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
